1、机器学习的一些概念：有监督、无监督、泛化能力、过拟合、欠拟合(方差和偏差以及各自解决办法)、交叉验证

有监督：在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。
无监督：在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。
泛化能力：泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。
过拟合：参数过多，导致在训练集上准确率很高，但换新样本会严重误判。
欠拟合：样本过少，无法归纳出足够的共性。
交叉验证：交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。

2、线性回归的原理
 线性回归可分为一元回归和多元回归，一元回归就是只有一个影响因子，也就是大家熟悉的线性方程，多元回归就是有多个影响因子。一元线性回归方程是 ，简单来说就是二维平面上的一条直线，多元线性回归方程 ，从数学角度如果只有两个影响因子可以看做是回归平面，如果涉及到高维度不能用图形来描述。

3、线性回归损失函数、代价函数、目标函数
这个损失函数用的是预测值与真实值之差的平方和。线性回归误差平方损失极小化与极大似然估计等价。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。

4、优化方法(梯度下降法、牛顿法、拟牛顿法等）

梯度下降法：梯度下降法是一个最优化算法，通常也称为最速下降法。最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。可以用于求解非线性方程组。
牛顿法：参考https://www.jianshu.com/p/0f864a4c3b38
拟牛顿法：参考https://www.jianshu.com/p/0f864a4c3b38

5、线性回归的评估指标

评价线性回归的指标有四种，均方误差（Mean Squared Error）、均方根误差（Root Mean Squared Error）、平均绝对值误差（Mean Absolute Error）以及R Squared方法。

6、sklearn参数详解
参考：https://www.jianshu.com/p/c6bfc9052325


